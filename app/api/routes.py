from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional

# Update your imports here
from app.llm.tools import CodeLanguageDetectionTool
from app.llm.chains import (
    create_code_explanation_chain,
    create_code_generation_chain,
    create_code_translation_chain
)
# Add this import
from app.llm.modelRegistry import get_model_for_task
from app.llm.modelTask import ModelTask
from app.utils.styleManager import StylePreferences, style_manager

router = APIRouter(prefix="/api", tags=["code"])

language_detector = CodeLanguageDetectionTool()

class CodeRequest(BaseModel):
    code: str
    language: str = None  # Keep your existing field definition

class ExplanationResponse(BaseModel):
    explanation: str
    language: str

class GenerationResponse(BaseModel):
    code: str
    language: str

class TranslationRequest(BaseModel):
    code: str
    target_language: str

# Add this class since you're using direct parameters in generate_code
class GenerationDescriptionRequest(BaseModel):
    description: str
    language: str

# Define API endpoints
@router.post("/explain_code", response_model=ExplanationResponse)
async def explain_code(request: CodeRequest):
    # If language is not provided, detect it
    detected_language = request.language
    if not detected_language:
        detected_language = language_detector._run(request.code)

    try:
        # Replace placeholder with actual chain call
        explanation_chain = create_code_explanation_chain()
        result = explanation_chain({
            "code": request.code,
            "language": detected_language
        })
        
        return {
            "explanation": result["explanation"],
            "language": detected_language
        }
    except Exception as e:
        # Fallback to your placeholder in case of error
        return {
            "explanation": f"Explanation for the provided {detected_language} code\nError: {str(e)}", 
            "language": detected_language
        }

# Keep your parameter structure but add request model handling
@router.get("/generate_code", response_model=GenerationResponse)
async def generate_code(description: str, language: str):
    try:
        # Get current style preferences
        prefs = style_manager.get_preferences_dict()
        
        # Create and run the generation chain
        generation_chain = create_code_generation_chain()
        
        # Use the new invocation style
        result = generation_chain({
            "description": description,
            "language": language,
            **prefs
        })
        
        # Debug print
        print(f"Generated code result: {result}")
        
        return {
            "code": result["code"] if result["code"] else f"// {language} code for: {description}\n// (No code generated by AI model)",
            "language": language
        }
    except Exception as e:
        # Fallback to your placeholder in case of error
        print(f"Generation error: {str(e)}")
        return {
            "code": f"// Generated {language} code\n// Based on: {description}\n// Error: {str(e)}", 
            "language": language
        }

@router.post("/translate_code", response_model=GenerationResponse)
async def translate_code(request: TranslationRequest):
    try:
        # Detect source language
        source_language = language_detector._run(request.code)
        
        # Get current style preferences
        prefs = style_manager.get_preferences_dict()
        
        # Create and run the translation chain
        translation_chain = create_code_translation_chain()
        
        # Use the new invocation style
        result = translation_chain({
            "code": request.code,
            "source_language": source_language,
            "target_language": request.target_language,
            **prefs
        })
        
        return {
            "code": result["translated_code"],
            "language": request.target_language
        }
    except Exception as e:
        # Fallback to your placeholder in case of error
        return {
            "code": f"// Translated code in {request.target_language}\n// Error: {str(e)}", 
            "language": request.target_language
        }

@router.post("/style_preferences", response_model=StylePreferences)
async def set_style_preferences(preferences: StylePreferences):
    # Replace placeholder with actual style manager
    try:
        style_manager.save_preferences(preferences)
    except Exception:
        pass  # Keep your placeholder behavior in case of error
    return preferences

# Add a GET endpoint for style preferences
@router.get("/style_preferences", response_model=StylePreferences)
async def get_style_preferences():
    """API endpoint to retrieve current style preferences"""
    try:
        return style_manager.load_preferences()
    except Exception:
        return StylePreferences()  # Return defaults in case of error

@router.get("/test_llm")
async def test_llm():
    """Simple endpoint to test direct LLM communication"""
    try:
        from app.llm.modelRegistry import get_model_for_task
        from app.llm.modelTask import ModelTask
        
        # Get the code generation model
        llm = get_model_for_task(ModelTask.CODE_GENERATION)
        
        # Try a simple prompt
        response = llm.invoke("Write a one-line Python print statement saying 'Hello from Ollama'")
        
        return {"success": True, "response": response}
    except Exception as e:
        return {"success": False, "error": str(e)}